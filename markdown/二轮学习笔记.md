# 机器学习二轮考核

[toc]

## May 1st Sat

### git

- 连接了TopView的gitea并建立仓库，为二轮学习做好了准备。

> github一直因为某种原因（大家都懂）做不了图床，这个用起来真香

![gitea](https://git.topviewclub.cn/ml-lilong/pictures/raw/branch/master/gitea_pic1.jpg)

顺便记录一下git的常用命令

```
git init  # 初始化
git add .  # 添加文件
git commit -m ""  # 提交
git push  # 推送至远程仓库
git status  # 查看状态
git remote rm origin  # 删除远程仓库的连接
git remote add origin xxx  # 添加远程仓库连接
git pull  # 拉取仓库
```

### SoftMax Regression

- SoftMax和线性回归一样，相当于是一个单层的神经网络。

- 假设函数:

  其中k为类别总数，i为样本，j为类别。

$$
h_\theta(x^{(i)})=\frac{1}{\sum^k_{j=1}e^{\theta^T_j x^{(i)}}}\cdot \begin{bmatrix}e^{\theta^T_1 x^{(i)}}
\\ e^{\theta^T_2 x^{(i)}}
\\ ...
\\ e^{\theta^T_k x^{(i)}}
\end{bmatrix}
$$

这里假设函数由左右两式相乘得到，

右式中的每一个值，分别是将样本分类到k的概率。

其中左式起到是一个归一化作用，使右式所有概率总和为1。

- 代价函数：

  m为样本总数，k为类别总数，i为样本，l和j为类别。

$$
J(\theta)=-\frac{1}{m}[\sum^m_{i=1} \sum^k_{j=1}I\{y^{(i)}=j\}log\frac{e^{\theta^T_j x^{(i)}}}{\sum^k_{l=1}e^{\theta^T_l x^{(i)}}}]
$$

$$
（其中 I\{y^{(i)}=j\}为示性函数，\{\}内值为真时为1，否则为0）
$$

- 求偏导后，梯度下降公式：

$$
\theta=\theta-\frac{\alpha}{m}[\sum^m_{i=1}x^{(i)}(\frac{e^{\theta^T_j x^{(i)}}}{\sum^k_{l=1}e^{\theta^T_l x^{(i)}}}-I\{y^{(i)}=j\})]
$$

- 代码实现：

  最后使用了一次for循环作为不同类别的分别迭代（起到示性函数的作用），

  然后使用矩阵运算梯度下降，由于用了for，速度会慢一点，但经过测试，最终的分类效果不错。

## May 2nd Sun

### 朴素贝叶斯

- 朴素贝叶斯模型属于生成模型，它的基本假设是条件独立性。

- 朴素贝叶斯的条件独立性假设：

  （k为类别，x为输入的特征向量，j为向量中的元素）

$$
P(X=x|Y=c_k)=\prod_{j=1}^nP(X^{(j)}=x^{(j)}|Y=c_k)
$$

- 贝叶斯定理计算后验概率：

  (k为类别)

$$
P(Y=c_k|X=x)=\frac{P(X=x|Y=c_k)P(Y=c_k)}{\sum_k P(X=x|Y=c_k)P(Y=c_k)}
$$

- 最后得出贝叶斯分类器：
  $$
  y=argmax_{c_k}P(Y=c_k)\prod_jP(X^{(j)}=x^{(j)}|Y=c_k)
  $$

- 算法实现

  - 先算出先验概率

    （N为样本总数）
    $$
    P(Y=c_k)=\frac{\sum_{i=1}^NI(y_i=c_k)}{N}
    $$
  
- 再计算所有的条件概率
  
  （ajl为特征xj的所有出现的取值）
  $$
    P(X^{(j)}=a_{jl}|Y=c_k)=\frac{\sum_{i=1}^NI(x_i^{(j)}=a_{jl},y_i=c_k)}{\sum_{i=1}^NI(y_i=c_k)}
  $$
  
  - 最后用分类器进行预测

- 由于存在可能因为训练集不充分等原因，导致概率估值为0的情况发生，所以在条件概率计算时引入拉普拉斯修正：

  （Sj为特征xj的所有取值总数）
  $$
  P_1(X^{(j)}=a_{jl}|Y=c_k)=\frac{\sum_{i=1}^NI(x_i^{(j)}=a_{jl},y_i=c_k)+1}{\sum_{i=1}^NI(y_i=c_k)+S_j}
  $$

## May 3rd Mon

### 朴素贝叶斯

- 代码实现：

  首先，朴素贝叶斯分类的流程是：

  - 先算出先验概率

    - 我使用了字典式储存，方便预测时调用

  - 再算出所有的条件概率

    - 这里我用了三重for循环，也是字典式的储存， 实在没有想到别的更好的方法了（用数组储存的话，怕会后面调用的时候调错）

    - 在计算条件概率时加入了拉普拉斯平滑

  - 最后根据已有的先验概率和条件概率，直接计算得出各个y分类的置信概率：
    $$
    y=argmax_{c_k}P(Y=c_k)\prod_jP(X^{(j)}=x^{(j)}|Y=c_k)
    $$

    - 因为存在由训练集不充分等原因，导致部分条件概率值为0，

    - 在代码中的具体体现，就是储存的条件概率字典中不存在这个key

    - 因此使用if判断，当key不存在时，手动赋予概率，公式为：
      $$
      P_1(X^{(j)}=a_{jl}|Y=c_k)=\frac{1}{\sum_{i=1}^NI(y_i=c_k)+S_j}
      $$

  - 然后以置信概率最大的分类作为x的分类。

  - 最后由于每次预测都需要查字典，所以只能一次预测一个值，因此需要使用for循环遍历实现预测。

## May 4th Tue

### 支持向量机

支持向量机是一种二分类模型（也可用于回归）

它的原理是不同类别之间， 取间隔最大的一个超平面进行线性划分。

![SVM](https://git.topviewclub.cn/ml-lilong/pictures/raw/branch/master/svm_linear.jpg)

- 线性可分支持向量机

  - 采用硬间隔划分，间隔内不允许存在不同类的点。
  - 用于数据线性可分的时候。

- 线性支持向量机

  - 软间隔划分，允许存在少量的点存在于间隔内。
  - 用于数据近似线性可分的时候。

- 非线性支持向量机

  - 使用核函数，使支持向量机可学习非线性数据。
  - 同时也采用了软间隔划分。

- 超平面和数据集中的任一样本的几何间隔：
  $$
  \gamma_i=y_i(\frac{w}{\|w\|}\cdot x_i+\frac{b}{\|w\|})
  $$

  - 几何间隔除以||w||使超平面法向量模长为1，如果不除的话就是函数间隔。
  - 超平面和数据集的几何间隔为，所有样本点的几何间隔的最小值。

- 支持向量机最优化求解，转化为超平面条件约束下，求解间隔的最大值。
  $$
  max_{w,b}\space\space \frac{1}{\|w\|}
  $$

  $$
  s.t.\space\space y_i(w\cdot x_i+b)\geq 1
  $$

  - 等同于求解：

  $$
  min_{w,b}\space\space \frac{1}{2}\|w\|^2
  $$

  $$
  s.t.\space\space y_i(w\cdot x_i+b)\geq 1
  $$

  + 对于软间隔划分，加上一个松弛变量：

  $$
  min_{w,b}\space\space \frac{1}{2}\|w\|^2+C\sum^N_{i=1}\xi_i\space
  (C为惩罚参数)
  $$

  $$
  s.t.\space\space y_i(w\cdot x_i+b)\geq 1-\xi_i
  $$

  

- 求解w和b可以利用拉格朗日乘子法：
  $$
  L(w,b,\alpha)=\frac{1}{2}\|w\|^2+\sum^N_{i=1}\alpha_i(1-y_i(w\cdot x_i+b)
  $$

  - 转换为求解：

  $$
  min_\alpha \space\space \frac1 2 \sum^N_{i=1 }\sum^N_{j=1}\alpha_i\alpha_jy_iy_j(x_i^Tx_i)-\sum^N_{i=1}\alpha
  $$

  $$
  s.t. \space\space \sum^N_{i=1}\alpha_iy_i=0
  $$

  - 选出一个大于0的拉格朗日乘子，再根据：(软间隔下拉格朗日乘子还需小于惩罚参数C)

  $$
  w^*=\sum^N_{i=1}\alpha^*_iy_ix_i
  $$

  $$
  b^*=y_j-\sum^N_{i=1}\alpha_i^*y_i(x_i^Tx_j)
  $$

  - 求出超平面和决策函数。


## May 5th Wed

### 支持向量机

- 对于非线性支持向量机，采用核方法，具体是将输入空间通过核函数映射到特征空间，使转化为线性问题解决。通过核技巧，可以不需要求出特征空间和映射函数，只需知道核函数就好了。

- 核函数：

  - 高斯核
    $$
    K(x,z)=exp(-\frac{\|x-z\|^2}{2\sigma^2})
    $$

  - 多项式核
    $$
    K(x,z)=(x\cdot z+C)^p
    $$

- 

## May 6th Thu

### 支持向量机

- 初步了解SMO算法

## May 7th Fri

### 支持向量机

- SMO算法（序列最小最优算法）求解凸二次规划对偶问题：
  $$
  min_\alpha \space\space \frac1 2 \sum^N_{i=1 }\sum^N_{j=1}\alpha_i\alpha_jy_iy_jK(x_i,x_i)-\sum^N_{i=1}\alpha
  $$

  
  $$
  s.t. \space\space \sum^N_{i=1}\alpha_iy_i=0\\
  s.t\space \space 0<a_i<C
  $$

- 首先目的是要求解所有的乘子
  $$
  a_i=\{a_1+a_2+...+a_n\}
  $$
  的最小值，于是每次从所有的ai中抽取任意两个a1和a2作为变量，剩余固定不变，转化为求解子问题而求得原问题的解

  - 子问题：

  $$
  min_{a_1,a_2}\space\space W(a_1,a_2)=\frac{1}{2}K_{11}a_1^2+\frac{1}{2}K_{22}a_2^2+y_1y_2K_{12}a_1a_2-(a_1+a_2)\\+y_1a_1\sum_{i=3}^N y_ia_iK_{i1}+y_2a_2\sum_{i=3}^Ny_ia_iK_{i2}+const
  $$

  $$
  \space \space K_{ij}=K(x_i,x_j)\\
  s.t\space \space a_1y_1+a_2y_2=-\sum_{i=3}^Ny_ia_i=const\space k\\
  0<a_i<C
  $$

  需满足的KTT条件：
  $$
  a_i=0 \Leftrightarrow y_i\sum_{i=1}^Na_jy_jK(x_j,x_i)+b\geq 1\\
  a_i=C \Leftrightarrow y_i\sum_{i=1}^Na_jy_jK(x_j,x_i)+b\leq 1\\
  0<a_i<C \Leftrightarrow y_i\sum_{i=1}^Na_jy_jK(x_j,x_i)+b= 1
  $$

- 其中一个a选取违犯KKT条件最严重的，另一个a是根据s.t.约束条件选取。

  先算出约束边界：

  (当y1!=y2时)
  $$
  L=max(0,a_2^{old}-a_1^{old}),\space H=min(C,C+a_2^{old}-a_1^{old})\\
  (其中a_2^{old}-a_1^{old}为k)
  $$
  (当y1==y2时)
  $$
  L=max(0,a_2^{old}+a_1^{old}-C),\space H=min(C,a_2^{old}+a_1^{old})\\
  (其中a_2^{old}+a_1^{old}为k)
  $$
  以a2作为求解变量，通过计算：
  $$
  g(x)=\sum_{i=1}^Na_iy_iK(x_i,x)+b\\
  E_i=g(x_i)-y_i=(\sum_{j=1}^Na_jy_iK(x_j,x_i)+b)-y_i
  $$
  可以求得预测值与真实值的误差E。

  再解出a2的不约束最优解：
  $$
  a_2^{new}=a_2^{old}+\frac{y_2(E_1-E_2)}{\eta}\\
  \eta=K_{11}+K_{22}-2K_{12}
  $$
  最后再进行约束：
  $$
  a_2^{new}=\left\{\begin{matrix}H,\space\space a_2^{new}>H\\a_2^{new},\space \space L\leq a_2^{new}\leq H\\L,\space \space a_2^{new}<L\end{matrix}\right.
  $$
  再由a2求得a1:
  $$
  a_1^{new}=a_1^{old}+y_1y_2(a_2^{old}-a_2^{new})
  $$
  接着计算新的截距b:
  $$
  b_1^{new}=-E_1-y_1K_{11}(a_1^{new}-a_1^{old})-y_2K_{21}(a_2^{new}-a_2^{old})+b^{old}\\
  b_2^{new}=-E_2-y_1K_{12}(a_1^{new}-a_1^{old})-y_2K_{22}(a_2^{new}-a_2^{old})+b^{old}
  $$
  当0<a<C，b1 == b2, 当a == 0或a == C时，取b1~b2之间的数皆可，故：
  $$
  b^{new}=\frac{b_1^{new}+b_2^{new}}{2}
  $$
  完成一次子问题求解

- 不断循环进行子问题求解，当所有的a都满足KKT约束时，优化便完成了

- 根据a计算权重w和截距b，从而得出超平面。

## May 8th Sat

### 支持向量机

代码实现和debug

## May 9th Sun

### 感知机

- 感知机模型：
  $$
  f(x)=sign(w\cdot x+b)
  $$
  和SVM相似，感知机也是找一个超平面进行分割正负类。

- 损失函数：
  $$
  L(w,b)=-\sum_{x_i\in M}y_i(w\cdot x_i+b)
  $$
  M为误分类的点的集合

- 感知机采用随机梯度下降：
  $$
  w \leftarrow w+\eta x_iy_i\\
  b \leftarrow b + \eta y_i
  $$

- 在代码实现过程中，采用给x加上一列1，使其变成增广矩阵，相当于b，然后可以转化为一个参数的迭代优化。

- 感知机的解是无穷多的，根据样本迭代顺序而得到不同的解。

- 感知机相当于一个单层神经网络只能实现线性可分的数据分类（因为是寻找超平面），

  对于线性不可分数据，要使用多层感知机，但感知机是仿射变换，不管多少次变换叠加在一起，也还是相当于一次变换，所以多少层感知机都相当于一层。

- 因此引入非线性的激活函数：

  - ReLU函数：
    $$
    ReLU(x)=max(0,x)
    $$
    ![ReLU](https://git.topviewclub.cn/ml-lilong/pictures/raw/branch/master/ReLU.jpg)
    
  - tanh函数--双曲正切：
    $$
    tanh(x)=\frac{e^x-e^{-x}}{e^x+e^{-x}}=\frac{1-e^{-2x}}{1+e^{-2x}}
    $$
    ![tanh](https://git.topviewclub.cn/ml-lilong/pictures/raw/branch/master/tanh.jpg)
    
  - sigmoid函数：
    $$
    sigmoid(x)=\frac{1}{1+e^{-x}}
    $$
    ![sigmoid](https://git.topviewclub.cn/ml-lilong/pictures/raw/branch/master/sigmoid.jpg)


### 多层感知机

![MLP](https://git.topviewclub.cn/ml-lilong/pictures/raw/branch/master/mlp.jpg)

- 多个感知机的输出结果，加上激活函数进行非线性变换后，作为下一层的感知机的输入，从而再次进行输出。

- 利用感知机预测的值与真实值的误差，计算代价，并求偏导进行反向传播更新权重矩阵，通过梯度迭代，最后可以得到训练好的感知机模型
- 多层感知机可能存在多个局部最小值，可以通过设置多个不同初始权重、模拟退火技术和随机梯度下降等方法，进行优化。

## May 10th Mon

### 支持向量机

代码修改。

### 多层感知机

代码实现MLP神经网络。

## May 11th Tue

### 局部线性回归

实际上就是线性回归，加上了一个权重，也称为局部加权线性回归
$$
\Theta=(X^TWX)^{-1}X^TWY
$$

## May 12th Wed

### 线性回归正则化

分为梯度下降的正则化和正规方程法的正则化。

正则化分为l1和l2，其中用于线性回归的一般是l2正则化，具有防止过拟合的作用。

- 梯度下降：

  代价函数加上正则化：
  $$
  J(\theta)=\frac{1}{2m}[\sum^m_{i=1}(h_\theta(x_i)-y_i)^2+\lambda\sum_{j=1}^n\theta_j^2]
  $$
  其中的λ称为正则化参数，通过修改λ的值，可以实现对模型的拟合程度进行控制，当λ很大每个θ的惩罚也很大，导致θ的值很小，所以最后模型可能会欠拟合。反之，当λ很小或为零时，对θ就没什么影响，随模型学习率更改，可能会达到过拟合。

- 正规方程：

  求解Θ的公式加上正则化变为：
  $$
  \Theta=\left (X^TX+\lambda\begin{bmatrix}0&&&\\&1&&\\&&&&\\&&...&\\&&&1\end{bmatrix}\right)^{-1}X^TY
  $$
  其中加上了一个第一位数字为0的单位矩阵和正则化参数，原因是没必要对第一个θ_0进行正则化。

  这样正则化后的正规方程法满足矩阵满秩，因此也可以用于防止矩阵不可逆。

  也称为岭回归。
  
- 加上完成的代码

## May 13th Thu

### 局部加权回归

![lowess](https://git.topviewclub.cn/ml-lilong/pictures/raw/branch/master/lowess.jpg)

- 代价函数
  $$
  J(\theta)=\frac1 2\sum_{i=1}^mw_i(h(x_i)-y_i)^2
  $$
  对θ求偏导数，经最小二乘法求解后：
  $$
  \theta=(X^TWX)^{-1}X^TWY
  $$
  其中w为对角矩阵

  w值的选取，采用高斯核计算：
  $$
  w_i=e^{-\frac{(x_i-x)^2}{2\sigma^2}}
  $$

- 完成了对单个值预测的代码

## May 14th Fri

### 局部线性回归

修改了好久好久的代码

- 利用np.newaxis增维，实现同时计算多个权重w
- 完成了一次性预测所有测试集的代码
- 结束了Lowess的代码

## May 15th Sat

### 自然语言处理

了解了分词、词向量以及文本分析大致流程

(然后去写选修作业去了

## May 16th Sun

### 无

又写了一天的选修（安卓编程）

## May 17th Mon

### 文本分类

下载了一个外卖评论.csv的文件，创建了ipynb文件开始进行分析

- jieba分词

  输入：

  ~~~python
  seg_list = jieba.cut("我来到北京清华大学", cut_all=True)
  print("Full Mode: " + "/ ".join(seg_list))  # 全模式
  
  seg_list = jieba.cut("我来到北京清华大学", cut_all=False)
  print("Default Mode: " + "/ ".join(seg_list))  # 精确模式
  
  seg_list = jieba.cut("他来到了网易杭研大厦")  # 默认是精确模式
  print(", ".join(seg_list))
  
  seg_list = jieba.cut_for_search("小明硕士毕业于中国科学院计算所，后在日本京都大学深造")  # 搜索引擎模式
  print(", ".join(seg_list))
  ~~~

  输出：

  ~~~python
  【全模式】: 我/ 来到/ 北京/ 清华/ 清华大学/ 华大/ 大学
  
  【精确模式】: 我/ 来到/ 北京/ 清华大学
  
  【新词识别】：他, 来到, 了, 网易, 杭研, 大厦
  
  【搜索引擎模式】： 小明, 硕士, 毕业, 于, 中国, 科学, 学院, 科学院, 中国科学院, 计算, 计算所, 后, 在, 日本, 京都, 大学, 日本京都大学, 深造
  ~~~

  修改分词：

  ~~~python
  # 使用del_word()使得某个词语不会出现
  jieba.del_word("")
  # 使用add_word()添加新词到字典中
  jieba.add_word("")
  # 使用suggest_freq()调整某个词语的词频，使得其在设置的词频高是能分出，词频低时不能分出
  jieba.suggest_freq("", True)
  ~~~

  jieba.cut 以及 jieba.cut_for_search 返回的结构都是一个可迭代的 generator，可以使用 for 循环来获得分词后得到的每一个词语(unicode)，或者用jieba.lcut 以及 jieba.lcut_for_search 直接返回 list.

## May 18th Tue

### 文本分类

通过调用Sklearn的CountVectorize实现分词、去除停用词和生成词频矩阵。

但是由于特征矩阵维度太大了，转换为数组时（ toarray() ）内存溢出。

解决方法：通过CountVectorize的max_features参数可以设置以出现次数最多的前n个词作为关键词库，这样可以缩小生成词频矩阵的维度。

## May 19th Wed

### 文本分类

借助sklearn的CountVectorizer对外卖评论进行分词、生成词频矩阵，再用朴素贝叶斯进行预测好评和差评。

## May 20th Thu

### 随机森林

- 随机森林是一种集成算法，这里用到的是bagging（boostrap aggregation

  boostrap称为自助法，是一种有放回的抽样方法，在随机森林中，使用多个决策树进行预测， 最后采取投票法选择预测结果。

- 首先先使用自助法从总样本中抽取多份大小相同的样本，分别给决策树进行训练，因为是有放回的抽样，所以存在相同的样本但又有不一样的。

  给我的感觉是，每棵树都以不同的角度来观察样本特征，既能看到共同的地方，又有自己特别的见解

- 其中还可以选择给每棵树随机的特征数，即不一定给出所有的特征，比如在极端条件下，可能每棵树各自分别代表一个特征（全部特征都给出也没有问题）

- 这样训练出来的决策树都有着自己的特性，在预测阶段就可能有着不同的预测结果，对此，回归求平均，分类投票法，即可达到集成学习的效果。

- 除了完成了随机森林的代码（其实就是用利用了已有的决策树模型改了一下），我还顺便修好了之前决策树的bug。

### 主成分分析

- 主成分分析（PCA）是一种无监督算法，可对数据进行降维。

- 首先先将数据0均值化，这样计算方差和协方差时可以减少运算量，

  将矩阵自己与自身转置相乘后，可发现，对角线上的值为每个特征的方差，其余元素为协方差，同时这是一个对称矩阵，如：
  $$
  X=\begin{pmatrix}a_1&a_2&...&a_m\\b_1&b_2&...&b_m\end{pmatrix}\\
  \frac{1}{m}XX^T=\begin{pmatrix}\frac{1}{m}\sum_{i=1}^ma_i^2&\frac{1}{m}\sum_{i=1}^ma_ib_i\\\frac{1}{m}\sum_{i=1}^ma_ib_i&\frac{1}{m}\sum_{i=1}^mb_i^2\end{pmatrix}
  $$
  这个称为协方差矩阵。

- 现在要使协方差都为0，方差尽可能大，所有要使协方差矩阵对角线以外的元素都为0，假设原矩阵X的协方差矩阵为C， 基变化后Y=PX，Y的协方差矩阵为D
  $$
  D=\frac{1}{m}YY^T\space\space\space\space\space\space\space\space\space\space\space\space\space\space\space\space\space\\=\frac{1}{m}(PX)(PX)^T\\=\frac{1}{m}PXX^TP^T\space\space\space\\=P(\frac{1}{m}XX^T)P^T\\=PCP^T\space\space\space\space\space\space\space\space\space\space\space\space\space\space
  $$
  所以现在要找一个P矩阵使PCP^T为对角化后的矩阵，而这个P矩阵其实就是由协方差矩阵的特征向量组成的矩阵，也叫奇异值矩阵，因此只要求得特征向量即可。

- 最后选择特征值最大的前k个向量组成基变化矩阵，既可以将原矩阵降维至k。

- 完成了PCA降维的代码



考核结束，完结撒花！:happy:

