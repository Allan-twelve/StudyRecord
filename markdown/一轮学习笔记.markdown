# 一轮考核总结

*时间：from 3.24 to 4.22*

[toc]

## 前言

​        本次考核，使用了**PyCharm**、**Jupyter lab**和**Jupyter notebook**进行代码实现和测试，参考学习资料有**周志华 西瓜书**、**李航 统计学习方法**，**python网络爬虫开发实战**和**python数据分析手册**等，学习途径有**CSDN**，**廖雪峰博客**，**吴恩达机器学习网课**，以及各大搜索引擎等。

​		本次考核我主要分前期、中期、后期三个阶段进行完成。

---

## 前期

​        考核前期主要是学习**机器学习算法**和**数据预处理方法**，过程大致是先初步学习了解原理，再进一步进行代码实现，最后再检验与完善代码并**封装成类**。

---

各算法实现顺序：

- 线性回归

  - 正规方程法
  - 梯度下降法
  - l2正则化

- 逻辑回归

  - 二分类
  - l2正则化

  - one vs all

- 决策树

  - ID3分类树
  - C4.5分类树
  - CART分类树

- K近邻

  - 分类

---

### 线性回归

#### 公式推导

---

##### 正规方程法

- 线性方程中的预测函数：（n为特征x总数）

$$
h_{\theta }(x)=\theta ^{T}x=\theta _{0}x_{0}+\theta _{1}x_{1}+\theta _{2}x_{2}+...+\theta _{n}x_{n}
$$

- 平方误差代价函数：

$$
X,Y,\Theta都是矩阵或向量
$$

$$
J(\theta )=(Y-X\Theta )^{2}
$$

$$
=(Y-X\Theta)^{T}\cdot (Y-X\Theta )
$$

$$
=(Y^{T}-\Theta ^{T}X^{T})\cdot (Y-X\Theta )
$$

$$
=Y^{T}Y-\Theta ^{T}X^{T}Y-Y^{T}X\Theta +\Theta ^{T}X^{T}X\Theta
$$

- 对代价函数求偏导数：

$$
\frac{\partial }{\partial \theta }J(\theta )=-X^{T}Y-X^{T}Y +2X^{T}X\Theta
$$

- 令J函数为0，化简：

$$
X^{T}Y=X^{T}X\Theta
$$

$$
\Theta=(X^{T}X)^{-1}X^{T}Y
$$

---

##### 代码实现：

​		主要利用numpy.dot()进行矩阵运算和numpy.linalg.inv()求逆，最后封装成类。

---

##### 梯度下降

- 线性方程中的预测函数：

$$
h_{\theta }(x)=\theta ^{T}x=\theta _{0}x_{0}+\theta _{1}x_{1}+\theta _{2}x_{2}+...+\theta _{n}x_{n}
$$

- 平方误差代价函数：(这里用2m方便计算)

$$
J(\theta )=\frac{1}{2m} \sum_{i=1}^{m}\left ( h_{\theta }(x_{i})-y_{i} \right )^{2}
$$

- 梯度下降的一般更新公式为：

$$
\theta _{j}=\theta _{j}-\alpha \frac{\partial }{\partial \theta _{j}}J (\theta )
$$

- 对代价函数求偏导数：

$$
\frac{\partial J (\theta )}{\partial \theta _{j}}=\frac{\partial J (\theta )}{\partial h_{\theta }(x)}\cdot \frac{\partial h_{\theta }(x)}{\partial \theta _{j}}
$$

$$
\frac{\partial J (\theta )}{\partial h_{\theta }(x)}=\frac{1}{m}\sum_{i=1}^{m}(h_{\theta }(x_{i})-y_{i})
$$

$$
\frac{\partial h_{\theta }(x)}{\partial \theta _{j}}=x_{j}
$$

$$
\frac{\partial J (\theta )}{\partial \theta _{j}}=\frac{1}{m}\sum_{i=1}^{m}(h_{\theta }(x_{i})-y_{i})x_{ij}
$$

- 得出梯度更新公式：

$$
\theta _{j}=\theta _{j}-\alpha \frac{1}{m}\sum_{i=1}^{m}(h_{\theta }(x_{i})-y_{i})x_{ij}
$$

---

##### 代码实现：

​		主要通过合成特征矩阵X一次性输入全部样本进行模型训练，利用for循环进行固定次数迭代，最后封装成类。

---

### 逻辑回归

#### 公式推导

- 预测函数为sigmoid函数：

$$
h_{\theta }(x)=\frac{1}{1+e^{-Z_{\theta }(x)}}
$$

$$
Z_{\theta }(x)=\theta ^{T}x=\theta _{0}x_{0}+\theta _{1}x_{1}+\theta _{2}x_{2}+...+\theta _{n}x_{n}
$$

- 代价函数为：

$$
J(\theta)=-\frac{1}{m}\sum_{i=1}^{m}(y_{i}logh_{\theta }(x)+(1-y_{i})log(1-h_{\theta }(x)))
$$

- 梯度一般更新公式为：

$$
\theta _{j}=\theta _{j}-\alpha \frac{\partial }{\partial \theta _{j}}J (\theta )
$$

- 求代价函数的偏导数（链式求导）：

$$
\frac{\partial }{\partial \theta _{j}}J (\theta )=\frac{\partial J(\theta)}{\partial h_{\theta}(x)}\cdot \frac{\partial h_{\theta}(x)}{\partial Z_{\theta}(x)}\cdot \frac{\partial Z_{\theta}(x)}{\partial \theta_j}
$$

$$
\frac{\partial J(\theta)}{\partial h_{\theta}(x)}=-\frac{1}{m}\sum_{i=1}^{m}(\frac{y_{i}}{h_{\theta}(x)}-\frac{1-y_{i}}{1-h_{\theta}(x)})----1
$$

$$
\frac{\partial h_{\theta}(x)}{\partial Z_{\theta}(x)}=\frac{e^{-Z_{\theta}(x)}}{(1+e^{-Z_{\theta}(x)})^{2}}----2
$$

$$
=\frac{1}{1+e^{-Z_{\theta}(x)}}\cdot \frac{e^{-Z_{\theta}(x)}}{1+e^{-Z_{\theta}(x)}}
$$

$$
=\frac{1}{1+e^{-Z_{\theta}(x)}}\cdot (1-\frac{1}{1+e^{-Z_{\theta}(x)}})
$$

$$
=h_{\theta}(x)\cdot (1-h_{\theta}(x))
$$

$$
\frac{\partial Z_{\theta}(x)}{\partial \theta_{j}}=x_{j}----3
$$

1，2，3式相乘：
$$
\frac{\partial }{\partial \theta _{j}}J (\theta )=-\frac{1}{m}\sum_{i=1}^{m}(y_{i}(1-h_{\theta}(x))-(1-y_{i})h_{\theta}(x))x_{i}
$$

$$
=\frac{1}{m}\sum_{i=1}^{m}(h_{\theta}(x)-y_{i})x_{i}
$$

很神奇，和线性回归如此相似，所以梯度公式为：
$$
\theta _{j}=\theta _{j}-\alpha \frac{1}{m}\sum_{i=1}^{m}(h_{\theta}(x)-y_{i})x_{i}
$$

##### 代码说明：

​		在已有的线性模型基础上进行修改， 加入了sigmoid函数，并实现了one VS all以用来解决**多分类**问题。

---

### 正则化

​		为了防止模型过拟合，以及正规方程的特征矩阵为不可逆矩阵导致不可运算，加入了l2正则化，并修改了代码。

---

### 决策树

- IDE3
  - 计算香农熵，代表着该变量的不确定性，熵越大，不确定性越大
  - 根据不同特征的条件下，求出各个特征的熵，根据不同条件下熵的减少程度，作为信息增益，而选取**最大信息增益**的特征作为决策树第一个分支。
- C4.5
  - 相比于IDE3，将**信息增益率**替代了信息增益作为决策标准
  - 信息增益率除于了特征取值的个数，可以避免因数据不平衡导致的决策结果不准确。
- CART
  - 分类回归树，使用**基尼指数**作为决策标准
  - 分别计算出经不同特征分割后的不确定性（我觉得就是看能不能有效的进行高纯度的正负类分类）

---

#### 代码说明

​		采用了递归字典式储存，效率比较低，且预测函数一次只能预测一个样本。

没有实现预剪枝以及后剪枝（只加了个决策阈值)

---

### K近邻

​		K近邻没有训练数据的过程，是直接根据已有的数据，计算如欧式距离，利用投票法，在k个最近邻样本中选择多数的进行归类。

#### 代码说明

​		用fit直接保存数据，predict计算距离并分类，利用了numpy的广播，最后封装成类。

---

### 数据分析库

深入学习了**numpy**，**pandas**，**matplotlib**等第三方库。

---

#### 数据预处理库

学习各种数据预处理方法的原理，并用代码实现了数据预处理库，封装成类。

- 归一化
  - 实现了利用极值进行区间缩放的MinMaxScale方法。
  - 区间缩放可以使各特征数据分布在0-1区间，使**梯度下降速度**加快， 更易达到局部最优。

- 标准化
  - 实现了Standardization方法。
  - 将数据归一化成均值为0，方差为1的数据，一般用于符合**高斯分布**的数据效果会比较好。
- 正则化
  - 虽然考核没有要求，但还是顺便实现了
  - 根据不同的范数可以进行不同的正则化
- 标签编码
  - 实现了LabelEncode将object等类型数据进行int型的**标签化**
  - 特点是存在分级，如1，2，3，4。

- 独热编码
  - 实现了OneHotEncode将object类型数据进行独热编码
  - 相比于标签编码，不同类别直接相互独立，**没有大小等级关系**
- 连续值离散化
  - 利用pandas模块的cut方法，封装了Cut方法
  - 可以将数据分区间切割
  - 实现了Binarize方法，根据阈值进行**二值化**（0或1）
- 缺失值处理
  - 利用了pandas的fillna进行封装，实现FillNan方法
  - 可以用不同方法填充缺失值
  - 也可以用pandas.dropna()删去缺失值
- 异常值检测和处理
  - 利用箱线图检查异常值
  - 并实现了Clean方法，根据箱线图原理（偏离四等分点1.5倍均值的数据视为异常值）进行清除数据

---

#### 数据分割库

- 留出法分割数据集
  - 实现了train_test_split函数，按照选择的比例，随机分割数据集
- 自助法
  - 实现了bootstrapping函数，可进行有放回的选取数据集
- 交叉验证法
  - **这个好难，代码写了好久**
  - 实现了自选分割多少份数据，进行多次自选模型训练并自选函数进行评估，最后返回评分的均值。

---

#### 模型评价库

##### 回归模型评估：

原理就是那样了，代码实现没有多少难点。

- MAE
  - 实现了**平均绝对误差**评估函数

- MSE
  - 实现了**平均平方误差**评估函数

- RMSE
  - 实现了**均方根误差**评估函数

- MAPE
  - 实现了**平均绝对百分比**误差

---

##### 分类评估：

- 准确率
  - 预测正确的数量占总数量的比例

混淆矩阵里面有四个情况：

- FP：负类被预测为正类 

- TP：正类被正确预测

- FN：正类被预测为负类

- TN：负类被正确预测

- PR曲线
  - 根据召回率和准确率绘图
  - 图像越靠近右上角，说明模型越好
- ROC曲线
  - 越靠近左上角，模型越好
- 曲线下的面积就是AUC的值（这个还没有代码实现）
- F1-Score
  - 召回率和准确率的一种加权平均
  - 评分比较综合

至此，数据分析所需要的库与模型基本准备完毕。

---

## 中期

### 数据分析

​		应用**pandas**和**matplotlib**等第三方库和之前做好的**数据分析库**对*house-prices-advanced-regression-techniques* ，*GiveMeSomeCredit*两个数据集进行了数据分析，并分别用回归模型和分类模型进行训练与预测。

​		其中包括应用数据预处理，特征选择，数据分割，模型训练等等。并尝试多种可视化，使更加直观地分析数据。

​		具体分析都在ipynb文件中。

---

## 后期

### 爬虫

​		写了一个基于requests库的简单的爬虫接口类，爬取国家统计局的第六次人口普查的部分数据。

​		这个网站不但没有https安全协议，还没有robots.txt文件。。。

​		使用requests库和BeautifulSoup以及正则表达式进行网页解析与数据爬取，并储存为csv文件

​		使用了seaborn和Plotly对数据进行了可视化，分析并得出结论。

### 其他

- 增强了代码的健壮性，部分代码增加了错误捕捉。
- 完善了代码的文档字符串（docstrings）并添加部分注释。
- 增加了与sklearn库的算法模型以及评估函数的对比。
- 整理笔记，并将代码合理分包。

---

## 后记

​		通过这次考核，我学到了很多新的知识，也算是第一次自己完整的进行了数据分析。也了解到很多新的第三方库，有了很多新的目标。期间遇到过很多问题，各种bug，甚至最后迫不得已重装anaconda... 不过，在我不懈地努力下，最后问题基本都解决了，我勉勉强强完成了考核，最大的遗憾就是唯一的无监督算法k-means还没能实现... ...